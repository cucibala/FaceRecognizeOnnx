#include "face_recognizer.h"
#include <cmath>
#include <iostream>

FaceRecognizer::FaceRecognizer(bool useGPU, int deviceId)
    : env_(ORT_LOGGING_LEVEL_ERROR, "FaceRecognizer"),  // æ”¹ä¸º ERROR çº§åˆ«ï¼Œå¿½ç•¥è­¦å‘Š
      session_(nullptr),
      inputWidth_(112),
      inputHeight_(112),
      featureDim_(512),
      useGPU_(useGPU),
      deviceId_(deviceId) {
    
    // å‡å°‘çº¿ç¨‹æ•°ä»¥é¿å…ä¸ OpenMP å†²çª
    sessionOptions_.SetIntraOpNumThreads(2);
    sessionOptions_.SetInterOpNumThreads(2);
    sessionOptions_.SetGraphOptimizationLevel(GraphOptimizationLevel::ORT_ENABLE_ALL);
    
    // å¦‚æœå¯ç”¨ GPUï¼Œé…ç½® GPU é€‰é¡¹
    if (useGPU_) {
        setupGPU();
    }
}

FaceRecognizer::~FaceRecognizer() {
    if (session_) {
        delete session_;
    }
}

bool FaceRecognizer::loadModel(const std::string& modelPath) {
    try {
        #ifdef _WIN32
            std::wstring wideModelPath(modelPath.begin(), modelPath.end());
            session_ = new Ort::Session(env_, wideModelPath.c_str(), sessionOptions_);
        #else
            session_ = new Ort::Session(env_, modelPath.c_str(), sessionOptions_);
        #endif
        
        // è·å–è¾“å…¥ä¿¡æ¯
        size_t numInputNodes = session_->GetInputCount();
        if (numInputNodes > 0) {
            Ort::AllocatedStringPtr inputNameAllocated = session_->GetInputNameAllocated(0, allocator_);
            inputNames_.push_back(std::string(inputNameAllocated.get()));
            
            Ort::TypeInfo inputTypeInfo = session_->GetInputTypeInfo(0);
            auto tensorInfo = inputTypeInfo.GetTensorTypeAndShapeInfo();
            inputShape_ = tensorInfo.GetShape();
            
            if (inputShape_.size() == 4) {
                // æ£€æŸ¥æ˜¯å¦ä¸ºåŠ¨æ€ç»´åº¦ï¼ˆ-1ï¼‰ï¼Œå¦‚æœæ˜¯åˆ™ä¿æŒé»˜è®¤å€¼
                int64_t modelHeight = inputShape_[2];
                int64_t modelWidth = inputShape_[3];
                
                if (modelHeight > 0) {
                    inputHeight_ = static_cast<int>(modelHeight);
                }
                if (modelWidth > 0) {
                    inputWidth_ = static_cast<int>(modelWidth);
                }
                
                std::cout << "Model input shape: [" << inputShape_[0] << ", " << inputShape_[1] 
                          << ", " << inputShape_[2] << ", " << inputShape_[3] << "]" << std::endl;
                if (modelHeight <= 0 || modelWidth <= 0) {
                    std::cout << "Note: Dynamic dimensions detected (-1), using default size: " 
                              << inputWidth_ << "x" << inputHeight_ << std::endl;
                }
            }
        }
        
        // è·å–è¾“å‡ºä¿¡æ¯
        size_t numOutputNodes = session_->GetOutputCount();
        for (size_t i = 0; i < numOutputNodes; i++) {
            Ort::AllocatedStringPtr outputNameAllocated = session_->GetOutputNameAllocated(i, allocator_);
            outputNames_.push_back(std::string(outputNameAllocated.get()));
        }
        
        // åˆ›å»ºæŒ‡é’ˆæ•°ç»„
        inputNamePtrs_.clear();
        for (const auto& name : inputNames_) {
            inputNamePtrs_.push_back(name.c_str());
        }
        
        outputNamePtrs_.clear();
        for (const auto& name : outputNames_) {
            outputNamePtrs_.push_back(name.c_str());
        }
        
        std::cout << "Face recognizer model loaded successfully!" << std::endl;
        std::cout << "Using input size: " << inputWidth_ << "x" << inputHeight_ << std::endl;
        
        // æ˜¾ç¤ºé…ç½®çš„ Provider
        std::cout << "Configured Execution Providers: ";
#ifdef USE_TENSORRT
        std::cout << "TensorRT ";
#endif
#ifdef USE_CUDA
        std::cout << "CUDA ";
#endif
#if !defined(USE_CUDA) && !defined(USE_TENSORRT)
        std::cout << "CPU ";
#endif
        std::cout << std::endl;
        
        if (useGPU_) {
#if defined(USE_CUDA) || defined(USE_TENSORRT)
            std::cout << "âœ“ GPU mode active (device: " << deviceId_ << ")" << std::endl;
#else
            std::cerr << "âš ï¸  WARNING: GPU requested but compiled without CUDA/TensorRT!" << std::endl;
            std::cerr << "    Recompile with: cmake -DUSE_CUDA=ON" << std::endl;
#endif
        }
        
        std::cout << "Number of outputs: " << outputNames_.size() << std::endl;
        
        return true;
    } catch (const Ort::Exception& e) {
        std::cerr << "Error loading face recognizer model: " << e.what() << std::endl;
        return false;
    }
}

cv::Mat FaceRecognizer::alignFace(const cv::Mat& image, const FaceBox& face) {
    // éªŒè¯è¾“å…¥
    if (image.empty()) {
        std::cerr << "Empty image in alignFace" << std::endl;
        return cv::Mat();
    }
    
    // æ ‡å‡†5ç‚¹æ¨¡æ¿ï¼ˆ112x112å›¾åƒï¼‰
    cv::Point2f dstLandmarks[5] = {
        cv::Point2f(38.2946f, 51.6963f),  // å·¦çœ¼
        cv::Point2f(73.5318f, 51.5014f),  // å³çœ¼
        cv::Point2f(56.0252f, 71.7366f),  // é¼»å­
        cv::Point2f(41.5493f, 92.3655f),  // å·¦å˜´è§’
        cv::Point2f(70.7299f, 92.2041f)   // å³å˜´è§’
    };
    
    // ä½¿ç”¨ç›¸ä¼¼å˜æ¢å¯¹é½äººè„¸
    cv::Mat M = cv::estimateAffinePartial2D(
        std::vector<cv::Point2f>(face.landmarks, face.landmarks + 5),
        std::vector<cv::Point2f>(dstLandmarks, dstLandmarks + 5)
    );
    
    // æ£€æŸ¥å˜æ¢çŸ©é˜µæ˜¯å¦æœ‰æ•ˆ
    if (M.empty()) {
        std::cerr << "Failed to compute affine transformation" << std::endl;
        // å¦‚æœæ— æ³•è®¡ç®—å˜æ¢ï¼Œç›´æ¥è£å‰ªå¹¶ç¼©æ”¾äººè„¸åŒºåŸŸ
        cv::Rect safeFace = face.box & cv::Rect(0, 0, image.cols, image.rows);
        if (safeFace.width > 0 && safeFace.height > 0) {
            cv::Mat cropped = image(safeFace);
            cv::Mat resized;
            cv::resize(cropped, resized, cv::Size(inputWidth_, inputHeight_));
            return resized;
        }
        return cv::Mat();
    }
    
    cv::Mat aligned;
    cv::warpAffine(image, aligned, M, cv::Size(inputWidth_, inputHeight_));
    
    return aligned;
}

void FaceRecognizer::preprocess(const cv::Mat& aligned, std::vector<float>& inputData) {
    // BGRè½¬RGB
    cv::Mat rgb;
    cv::cvtColor(aligned, rgb, cv::COLOR_BGR2RGB);
    
    // è½¬æ¢ä¸ºfloatå¹¶å½’ä¸€åŒ–åˆ°[-1, 1]
    inputData.resize(inputWidth_ * inputHeight_ * 3);
    for (int c = 0; c < 3; ++c) {
        for (int h = 0; h < inputHeight_; ++h) {
            for (int w = 0; w < inputWidth_; ++w) {
                inputData[c * inputHeight_ * inputWidth_ + h * inputWidth_ + w] = 
                    (rgb.at<cv::Vec3b>(h, w)[c] - 127.5f) / 128.0f;
            }
        }
    }
}

std::vector<float> FaceRecognizer::extractFeatureSimple(const cv::Mat& image) {
    std::vector<float> feature;
    
    if (!session_) {
        std::cerr << "Model not loaded!" << std::endl;
        return feature;
    }
    
    // éªŒè¯è¾“å…¥
    if (image.empty()) {
        std::cerr << "Input image is empty!" << std::endl;
        return feature;
    }
    
    // ç›´æ¥ resize åˆ°æ¨¡å‹è¾“å…¥å°ºå¯¸
    cv::Mat resized;
    cv::resize(image, resized, cv::Size(inputWidth_, inputHeight_));
    
    // é¢„å¤„ç†
    std::vector<float> inputData;
    preprocess(resized, inputData);
    
    // æ£€æŸ¥é¢„å¤„ç†ç»“æœ
    if (inputData.empty()) {
        std::cerr << "Preprocessing failed!" << std::endl;
        return feature;
    }
    
    // åˆ›å»ºè¾“å…¥tensor
    std::vector<int64_t> inputShapeBatch = {1, 3, inputHeight_, inputWidth_};
    auto memoryInfo = Ort::MemoryInfo::CreateCpu(OrtArenaAllocator, OrtMemTypeDefault);
    Ort::Value inputTensor = Ort::Value::CreateTensor<float>(
        memoryInfo, inputData.data(), inputData.size(),
        inputShapeBatch.data(), inputShapeBatch.size()
    );
    
    // æ¨ç†
    try {
        auto outputTensors = session_->Run(
            Ort::RunOptions{nullptr},
            inputNamePtrs_.data(), &inputTensor, 1,
            outputNamePtrs_.data(), outputNamePtrs_.size()
        );
        
        // è·å–ç‰¹å¾
        float* outputData = outputTensors[0].GetTensorMutableData<float>();
        auto outputShape = outputTensors[0].GetTensorTypeAndShapeInfo().GetShape();
        
        size_t featureSize = 1;
        for (auto dim : outputShape) {
            featureSize *= dim;
        }
        
        feature.assign(outputData, outputData + featureSize);
        
        // L2å½’ä¸€åŒ–
        normalize(feature);
        
    } catch (const Ort::Exception& e) {
        std::cerr << "Error during feature extraction: " << e.what() << std::endl;
    }
    
    return feature;
}

std::vector<float> FaceRecognizer::extractFeature(const cv::Mat& image, const FaceBox& face) {
    std::vector<float> feature;
    
    if (!session_) {
        std::cerr << "Model not loaded!" << std::endl;
        return feature;
    }
    
    // éªŒè¯è¾“å…¥
    if (image.empty()) {
        std::cerr << "Input image is empty!" << std::endl;
        return feature;
    }
    
    // å¯¹é½äººè„¸
    cv::Mat aligned = alignFace(image, face);
    
    // æ£€æŸ¥å¯¹é½ç»“æœ
    if (aligned.empty()) {
        std::cerr << "Face alignment failed!" << std::endl;
        return feature;
    }
    
    // é¢„å¤„ç†
    std::vector<float> inputData;
    preprocess(aligned, inputData);
    
    // æ£€æŸ¥é¢„å¤„ç†ç»“æœ
    if (inputData.empty()) {
        std::cerr << "Preprocessing failed!" << std::endl;
        return feature;
    }
    
    // åˆ›å»ºè¾“å…¥tensor
    std::vector<int64_t> inputShapeBatch = {1, 3, inputHeight_, inputWidth_};
    auto memoryInfo = Ort::MemoryInfo::CreateCpu(OrtArenaAllocator, OrtMemTypeDefault);
    Ort::Value inputTensor = Ort::Value::CreateTensor<float>(
        memoryInfo, inputData.data(), inputData.size(),
        inputShapeBatch.data(), inputShapeBatch.size()
    );
    
    // æ¨ç†
    try {
        auto outputTensors = session_->Run(
            Ort::RunOptions{nullptr},
            inputNamePtrs_.data(), &inputTensor, 1,
            outputNamePtrs_.data(), outputNamePtrs_.size()
        );
        
        // è·å–ç‰¹å¾
        float* outputData = outputTensors[0].GetTensorMutableData<float>();
        auto outputShape = outputTensors[0].GetTensorTypeAndShapeInfo().GetShape();
        
        size_t featureSize = 1;
        for (auto dim : outputShape) {
            featureSize *= dim;
        }
        
        feature.assign(outputData, outputData + featureSize);
        
        // L2å½’ä¸€åŒ–
        normalize(feature);
        
    } catch (const Ort::Exception& e) {
        std::cerr << "Error during feature extraction: " << e.what() << std::endl;
    }
    
    return feature;
}

void FaceRecognizer::normalize(std::vector<float>& feature) {
    float norm = 0.0f;
    for (float val : feature) {
        norm += val * val;
    }
    norm = std::sqrt(norm);
    
    if (norm > 0) {
        for (float& val : feature) {
            val /= norm;
        }
    }
}

std::vector<std::vector<float>> FaceRecognizer::extractFeaturesBatchSimple(const std::vector<cv::Mat>& images) {
    std::vector<std::vector<float>> features;
    
    if (!session_) {
        std::cerr << "Model not loaded!" << std::endl;
        return features;
    }
    
    if (images.empty()) {
        std::cerr << "No images to process!" << std::endl;
        return features;
    }
    
    int batchSize = images.size();
    try {
        // ===== æ—¶é—´æµ‹é‡ï¼šé¢„å¤„ç† =====
        auto tp1 = std::chrono::high_resolution_clock::now();
        
        // ä¼˜åŒ–ï¼šä¸€æ¬¡æ€§åˆ†é…æ‰€æœ‰å†…å­˜
        const int singleImageSize = 3 * inputHeight_ * inputWidth_;
        std::vector<float> batchInputData(batchSize * singleImageSize);
        std::vector<bool> validFlags(batchSize, false);
        
        // ä¸²è¡Œå¤„ç†ï¼Œé¿å…ä¸ ONNX Runtime çº¿ç¨‹å†²çª
        for (int i = 0; i < batchSize; i++) {
            int offset = i * singleImageSize;
            
            if (images[i].empty()) {
                continue;
            }

            // Resize
            cv::Mat resized;
            cv::resize(images[i], resized, cv::Size(inputWidth_, inputHeight_));
            
            // BGRè½¬RGBå¹¶å½’ä¸€åŒ–ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰
            cv::Mat rgb;
            cv::cvtColor(resized, rgb, cv::COLOR_BGR2RGB);
            
            // ç›´æ¥å†™å…¥æ‰¹æ¬¡æ•°æ®ï¼ˆé¿å…é¢å¤–çš„ vector å¤åˆ¶ï¼‰
            for (int c = 0; c < 3; ++c) {
                for (int h = 0; h < inputHeight_; ++h) {
                    for (int w = 0; w < inputWidth_; ++w) {
                        int idx = offset + c * inputHeight_ * inputWidth_ + h * inputWidth_ + w;
                        batchInputData[idx] = (rgb.at<cv::Vec3b>(h, w)[c] - 127.5f) / 128.0f;
                    }
                }
            }
            
            validFlags[i] = true;
        }
        
        auto tp2 = std::chrono::high_resolution_clock::now();
        auto preprocessTime = std::chrono::duration_cast<std::chrono::milliseconds>(tp2 - tp1).count();
        
        // ===== æ—¶é—´æµ‹é‡ï¼šåˆ›å»ºtensor =====
        auto tt1 = std::chrono::high_resolution_clock::now();
        
        std::vector<int64_t> inputShapeBatch = {static_cast<int64_t>(batchSize), 3, inputHeight_, inputWidth_};
        auto memoryInfo = Ort::MemoryInfo::CreateCpu(OrtArenaAllocator, OrtMemTypeDefault);
        Ort::Value inputTensor = Ort::Value::CreateTensor<float>(
            memoryInfo, batchInputData.data(), batchInputData.size(),
            inputShapeBatch.data(), inputShapeBatch.size()
        );
        
        auto tt2 = std::chrono::high_resolution_clock::now();
        auto tensorTime = std::chrono::duration_cast<std::chrono::milliseconds>(tt2 - tt1).count();
        
        // ===== æ—¶é—´æµ‹é‡ï¼šçº¯æ¨ç† =====
        auto t1 = std::chrono::high_resolution_clock::now();
        auto outputTensors = session_->Run(
            Ort::RunOptions{nullptr},
            inputNamePtrs_.data(), &inputTensor, 1,
            outputNamePtrs_.data(), outputNamePtrs_.size()
        );
        auto t2 = std::chrono::high_resolution_clock::now();
        auto inferenceTime = std::chrono::duration_cast<std::chrono::milliseconds>(t2 - t1).count();
        
        std::cout << "\n[Time Breakdown - batch=" << batchSize << "]" << std::endl;
        std::cout << "  Preprocessing: " << preprocessTime << " ms" << std::endl;
        std::cout << "  Tensor creation: " << tensorTime << " ms" << std::endl;
        std::cout << "  Pure inference: " << inferenceTime << " ms â­" << std::endl;
        std::cout << "  Total: " << (preprocessTime + tensorTime + inferenceTime) << " ms" << std::endl;
        
        std::cout << "==> Batch inference: " << inferenceTime << " ms (batch=" << batchSize << ")" << std::endl;
        std::cout << "    Per image: " << (inferenceTime * 1.0 / batchSize) << " ms" << std::endl;
        std::cout << "    Throughput: " << (batchSize * 1000.0 / inferenceTime) << " img/s" << std::endl;
        
        // æ€§èƒ½è¯Šæ–­
        if (useGPU_) {
            double perImageTime = inferenceTime * 1.0 / batchSize;
            std::cout << "\n[Performance Check]" << std::endl;
            
            if (perImageTime > 5.0) {
                std::cerr << "âŒ CRITICAL: Likely running on CPU!" << std::endl;
                std::cerr << "   Per-image time: " << perImageTime << " ms (expected <1ms on GPU)" << std::endl;
                std::cerr << "   Check:" << std::endl;
                std::cerr << "   1. ldd ./TestJniInterface | grep onnxruntime" << std::endl;
                std::cerr << "   2. ls $ONNXRUNTIME_DIR/lib/ | grep cuda" << std::endl;
                std::cerr << "   3. nvidia-smi (GPU should show activity)" << std::endl;
            } else if (perImageTime > 2.0) {
                std::cout << "âš ï¸  WARNING: Slower than expected for GPU" << std::endl;
                std::cout << "   Per-image: " << perImageTime << " ms (expected <1ms)" << std::endl;
            } else {
                std::cout << "âœ“ Performance looks good for GPU" << std::endl;
                std::cout << "   Per-image: " << perImageTime << " ms" << std::endl;
            }
        } else {
            double perImageTime = inferenceTime * 1.0 / batchSize;
            std::cout << "\n[CPU Mode]" << std::endl;
            std::cout << "   Per-image: " << perImageTime << " ms (typical CPU speed)" << std::endl;
        }
        
        // è·å–è¾“å‡º
        float* outputData = outputTensors[0].GetTensorMutableData<float>();
        auto outputShape = outputTensors[0].GetTensorTypeAndShapeInfo().GetShape();
        
        // è§£ææ¯ä¸ªå›¾ç‰‡çš„ç‰¹å¾
        if (outputShape.size() >= 2) {
            int outputBatchSize = static_cast<int>(outputShape[0]);
            int featureDim = static_cast<int>(outputShape[1]);
            
            for (int i = 0; i < outputBatchSize && i < batchSize; i++) {
                std::vector<float> feature;
                
                if (validFlags[i]) {
                    int offset = i * featureDim;
                    feature.assign(outputData + offset, outputData + offset + featureDim);
                    normalize(feature);
                }
                
                features.push_back(feature);
            }
        }
        
    } catch (const Ort::Exception& e) {
        std::cerr << "Error during batch feature extraction: " << e.what() << std::endl;
    }
    
    return features;
}

std::vector<std::vector<float>> FaceRecognizer::extractFeaturesBatch(const std::vector<cv::Mat>& images) {
    std::vector<std::vector<float>> features;
    
    if (!session_) {
        std::cerr << "Model not loaded!" << std::endl;
        return features;
    }
    
    if (images.empty()) {
        std::cerr << "No images to process!" << std::endl;
        return features;
    }
    
    // æ³¨æ„ï¼šç”±äºæ¯å¼ å›¾ç‰‡å¯èƒ½éœ€è¦ä¸åŒçš„å¯¹é½å‚æ•°ï¼Œ
    // è¿™é‡Œä½¿ç”¨é€ä¸ªå¤„ç†çš„æ–¹å¼ï¼ˆæœªæ¥å¯ä»¥ä¼˜åŒ–ä¸ºçœŸæ­£çš„æ‰¹å¤„ç†ï¼‰
    for (size_t i = 0; i < images.size(); i++) {
        std::vector<float> feature = extractFeatureSimple(images[i]);
        features.push_back(feature);
    }
    
    return features;
}

void FaceRecognizer::setupGPU() {
    try {
        std::cout << "Configuring GPU support..." << std::endl;
        
#ifdef USE_TENSORRT
        // TensorRT - å°è¯•æ·»åŠ ï¼Œå¤±è´¥åˆ™è·³è¿‡
        // å¦‚æœä¸æƒ³ç­‰å¾…å¼•æ“æ„å»ºï¼Œå¯ä»¥æ³¨é‡Šæ‰æ•´ä¸ª TensorRT å—ï¼Œåªç”¨ CUDA
        try {
            std::cout << "  Attempting to add TensorRT provider..." << std::endl;
            
            OrtTensorRTProviderOptions trt_options{};
            trt_options.device_id = deviceId_;
            trt_options.trt_engine_cache_enable = 1;
            trt_options.trt_engine_cache_path = "./trt_cache";
            
            // TensorRT è¯¦ç»†é…ç½®
            // trt_options.trt_max_workspace_size = 2ULL * 1024 * 1024 * 1024; // 2GB
            trt_options.trt_fp16_enable = 1;  // å¯ç”¨ FP16
            trt_options.trt_int8_enable = 0;   // ç¦ç”¨ INT8
            trt_options.trt_dla_enable = 0;    // ç¦ç”¨ DLA
            trt_options.trt_dump_subgraphs = 0;
            trt_options.trt_engine_decryption_enable = false;
            
            sessionOptions_.AppendExecutionProvider_TensorRT(trt_options);
            std::cout << "âœ“ TensorRT provider added successfully!" << std::endl;
            std::cout << "  Device: " << deviceId_ << std::endl;
            std::cout << "  FP16: Enabled" << std::endl;
            std::cout << "  Cache path: ./trt_cache" << std::endl;
            std::cout << "\nâš ï¸  IMPORTANT: TensorRT Engine Building Notes" << std::endl;
            std::cout << "  - First run with each NEW batch size: 1-5 min (building engine)" << std::endl;
            std::cout << "  - Subsequent runs: Fast (engine cached in ./trt_cache)" << std::endl;
            std::cout << "  - If program seems frozen during inference: WAIT, engine is building!" << std::endl;
        } catch (const Ort::Exception& e) {
            std::cerr << "âš ï¸  TensorRT provider failed: " << e.what() << std::endl;
            std::cerr << "   This is OK, will use CUDA instead" << std::endl;
        } catch (...) {
            std::cerr << "âš ï¸  TensorRT provider failed (unknown error)" << std::endl;
            std::cerr << "   This is OK, will use CUDA instead" << std::endl;
        }
#endif

#ifdef USE_CUDA
        // CUDA - æ›´ç¨³å®šçš„é€‰æ‹©
        try {
            OrtCUDAProviderOptions cuda_options;
            cuda_options.device_id = deviceId_;
            
            sessionOptions_.AppendExecutionProvider_CUDA(cuda_options);
            std::cout << "âœ“ CUDA provider added (device: " << deviceId_ << ")" << std::endl;
        } catch (const Ort::Exception& e) {
            std::cerr << "âš ï¸  Failed to add CUDA provider: " << e.what() << std::endl;
            std::cerr << "   Falling back to CPU" << std::endl;
        }
#endif

#if !defined(USE_CUDA) && !defined(USE_TENSORRT)
        std::cerr << "Warning: GPU requested but not compiled with CUDA/TensorRT support!" << std::endl;
        std::cerr << "Please recompile with: cmake -DUSE_CUDA=ON" << std::endl;
        std::cerr << "Using CPU execution" << std::endl;
#endif
        
    } catch (const std::exception& e) {
        std::cerr << "Error setting up GPU: " << e.what() << std::endl;
        std::cerr << "Using CPU execution" << std::endl;
    }
}

void FaceRecognizer::warmupTensorRT(const std::vector<int>& batchSizes) {
    if (!session_ || !useGPU_) {
        std::cout << "Skipping warmup: model not loaded or GPU not enabled" << std::endl;
        return;
    }
    
    std::cout << "\n========================================" << std::endl;
    std::cout << "  TensorRT Engine Warmup" << std::endl;
    std::cout << "========================================" << std::endl;
    std::cout << "Building TensorRT engines for batch sizes: ";
    for (size_t i = 0; i < batchSizes.size(); i++) {
        std::cout << batchSizes[i];
        if (i < batchSizes.size() - 1) std::cout << ", ";
    }
    std::cout << std::endl;
    std::cout << "This may take 1-5 minutes, but only runs once..." << std::endl;
    std::cout << std::endl;
    
    auto warmupStart = std::chrono::high_resolution_clock::now();
    
    for (int batchSize : batchSizes) {
        std::cout << "ğŸ“¦ Building engine for batch=" << batchSize << "..." << std::flush;
        auto start = std::chrono::high_resolution_clock::now();
        
        // åˆ›å»ºè™šæ‹Ÿè¾“å…¥æ•°æ®
        const int singleImageSize = 3 * inputHeight_ * inputWidth_;
        std::vector<float> dummyData(batchSize * singleImageSize, 0.0f);
        
        std::vector<int64_t> inputShapeBatch = {static_cast<int64_t>(batchSize), 3, inputHeight_, inputWidth_};
        auto memoryInfo = Ort::MemoryInfo::CreateCpu(OrtArenaAllocator, OrtMemTypeDefault);
        Ort::Value inputTensor = Ort::Value::CreateTensor<float>(
            memoryInfo, dummyData.data(), dummyData.size(),
            inputShapeBatch.data(), inputShapeBatch.size()
        );
        
        try {
            // è¿è¡Œä¸€æ¬¡æ¨ç†ä»¥è§¦å‘ TensorRT å¼•æ“æ„å»º
            auto outputTensors = session_->Run(
                Ort::RunOptions{nullptr},
                inputNamePtrs_.data(), &inputTensor, 1,
                outputNamePtrs_.data(), outputNamePtrs_.size()
            );
            
            auto end = std::chrono::high_resolution_clock::now();
            auto duration = std::chrono::duration_cast<std::chrono::milliseconds>(end - start).count();
            
            std::cout << " âœ“ Done in " << duration << " ms" << std::endl;
        } catch (const Ort::Exception& e) {
            std::cerr << " âœ— Failed: " << e.what() << std::endl;
        }
    }
    
    auto warmupEnd = std::chrono::high_resolution_clock::now();
    auto totalWarmupTime = std::chrono::duration_cast<std::chrono::milliseconds>(warmupEnd - warmupStart).count();
    
    std::cout << "\nâœ“ Warmup completed in " << totalWarmupTime << " ms" << std::endl;
    std::cout << "  All engines cached in ./trt_cache/" << std::endl;
    std::cout << "  Future runs will be fast!" << std::endl;
    std::cout << "========================================\n" << std::endl;
}

float FaceRecognizer::compareFaces(const std::vector<float>& feature1, const std::vector<float>& feature2) {
    if (feature1.size() != feature2.size() || feature1.empty()) {
        return 0.0f;
    }
    
    // è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
    float dotProduct = 0.0f;
    for (size_t i = 0; i < feature1.size(); i++) {
        dotProduct += feature1[i] * feature2[i];
    }
    
    // ç”±äºç‰¹å¾å·²ç»å½’ä¸€åŒ–ï¼Œä½™å¼¦ç›¸ä¼¼åº¦å°±æ˜¯ç‚¹ç§¯
    // å°†[-1, 1]æ˜ å°„åˆ°[0, 1]
    return (dotProduct + 1.0f) / 2.0f;
}

